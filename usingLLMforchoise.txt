Вот здесь начинается настоящая магия.
LLM перестаёт быть “читателем документов” и становится **юридическим навигатором**.

Ниже — как MCP и LLM вместе выбирают не документы, а **нужные смысловые блоки**.

---

# 1. Ключевая идея

LLM никогда не запрашивает:

> “дай мне документ”

Он запрашивает:

> “дай мне юридические фрагменты с таким смыслом”

Это возможно только потому что у тебя есть:

```
Document → Section → Chunk → Embedding
```

---

# 2. MCP Query Model

LLM формирует **семантический план**.

Пример запроса пользователя:

> «Могу ли я взыскать пеню с продавца за просрочку доставки?»

LLM превращает это в:

```
{
  intent: "consumer_penalty_delay",
  required_sections: [
    LAW_REFERENCES,
    COURT_REASONING,
    DECISION
  ],
  entities: [
    Consumer,
    Seller,
    LawArticle(ConsumerProtection),
    Penalty
  ]
}
```

---

# 3. MCP выполняет этот план

Шаги:

### A. Векторный поиск

По `LAW_REFERENCES` и `COURT_REASONING`
→ ищет chunks похожие по смыслу

### B. Entity filtering

Оставляет только те chunks, где:

* есть статьи закона
* есть стороны “продавец / потребитель”

### C. Outcome filtering

Смотрит в `DecisionOutcome`:

* только где потребитель выиграл

---

# 4. Результат, который получает LLM

Не текст, а пакет фактов:

```
[
 {
   case_id,
   court,
   judge,
   law_articles,
   reasoning_chunks[],
   decision_chunk,
   outcome
 }
]
```

Это уже:

> “прецеденты”, а не документы.

---

# 5. Почему это даёт сильнее ответы

LLM не гадает:

> «Наверное, закон такой…»

Она видит:

> «В 43 делах суды решили именно так»

И может сказать:

> “Суды Украины в 2023–2025 гг. в 78% случаев взыскивали пеню при задержке доставки более 10 дней.”

Это **невозможно без section-level MCP.**

---

# 6. Почему это не RAG, а Retrieval + Reasoning

Ты не просто достаёшь похожие куски.
Ты достаёшь **фрагменты с юридической ролью.**

FACTS ≠ DECISION ≠ LAW ≠ REASONING
И LLM это знает.

---

# 7. Это превращает систему в Legal Brain

Ты можешь:

* сравнивать аргументы судей
* находить типовые ошибки
* прогнозировать исход
* автоматически писать иски

Потому что LLM думает не PDF-файлами, а **структурой судебной практики.**

---

Если хочешь, следующим шагом могу показать:

* как выглядит MCP API для таких запросов
* или как делать scoring “вероятность выигрыша”
* или как автоматически генерировать процессуальные документы из этого слоя
